{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is adapted from this [Medium blog](https://medium.com/data-science-in-your-pocket/training-custom-ner-system-using-crfs-146e0e922851)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = pd.read_json(path_or_buf='../data/dev/train.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line if you don't have \"ru_core_news_md\"\n",
    "# !python -m spacy download ru_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hamadasalhab/Documents/innopolis-study-materials/S24/NLP/Assignments/nlp-assignment-3/.env/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import ru_core_news_md\n",
    "\n",
    "# Load Russian spacy nlp pipeline to extract features from the text\n",
    "ru_nlp = ru_core_news_md.load()\n",
    "\n",
    "# A method to extract the features from a given sentence from the dataset\n",
    "def extract_features(sentence):\n",
    "\n",
    "    def update_dict(word, pos):\n",
    "        if pos == \"previous\":\n",
    "            pos = \"-\"\n",
    "        elif pos == \"next\":\n",
    "            pos = \"+\"\n",
    "\n",
    "        postag = word.pos_\n",
    "        return {\n",
    "            f\"{pos}1:word.lower()\": word.text.lower(),\n",
    "            f\"{pos}1:word.istitle()\": word.text.istitle(),\n",
    "            f\"{pos}1:word.isupper()\": word.text.isupper(),\n",
    "            f\"{pos}1:postag\": postag,\n",
    "            f\"{pos}1:postag[:2]\": postag[:2],\n",
    "        }\n",
    "\n",
    "    # Pass the sentence through the pipeline\n",
    "    sentence = ru_nlp(sentence)\n",
    "    sentence_features = []\n",
    "    for i, token in enumerate(sentence):\n",
    "        postag = token.pos_\n",
    "\n",
    "        # Parameters from the Medium blog (achieved best performancee)\n",
    "        token_features = {\n",
    "            \"bias\": 1.0,\n",
    "            \"word.lower()\": token.text.lower(),\n",
    "            \"word[-3:]\": token.text[-3:],\n",
    "            \"word[-2:]\": token.text[-2:],\n",
    "            \"word.isupper()\": token.text.isupper(),\n",
    "            \"word.istitle()\": token.text.istitle(),\n",
    "            \"word.isdigit()\": token.text.isdigit(),\n",
    "            \"postag\": postag,\n",
    "            \"postag[:2]\": postag[:2],\n",
    "        }\n",
    "\n",
    "        if i == 0:\n",
    "            # Add beginning of sentence token\n",
    "            token_features[\"BOS\"] = True\n",
    "\n",
    "        else:\n",
    "            # Add previous word's features\n",
    "            token_features.update(update_dict(sentence[i - 1], pos=\"previous\"))\n",
    "        \n",
    "        if i < len(sentence) - 1:\n",
    "            # Add next word's features\n",
    "            token_features.update(update_dict(sentence[i + 1], pos=\"next\"))\n",
    "        else:\n",
    "            # Add end of sentence token\n",
    "            token_features[\"EOS\"] = True\n",
    "\n",
    "        sentence_features.append(token_features)\n",
    "\n",
    "    return sentence_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for training data: 100%|██████████| 519/519 [01:49<00:00,  4.74it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Generating features and labels\n",
    "labels_ = []\n",
    "features = []\n",
    "for idx, row in tqdm(train_df.iterrows(), total=len(train_df), desc=\"Extracting features for training data\"):\n",
    "    sentence, ners = row[\"sentences\"], row[\"ners\"]\n",
    "\n",
    "    doc = ru_nlp(sentence)\n",
    "    tokens = []\n",
    "    labels = ['O'] * len(doc)  # default label\n",
    "    for start, end, label in ners:\n",
    "        for token in doc:\n",
    "            if token.idx == start:\n",
    "                labels[token.i] = 'B-' + label  # beginning of entity\n",
    "            elif start < token.idx < end:\n",
    "                labels[token.i] = 'I-' + label  # inside of entity\n",
    "    features.append(extract_features(sentence))\n",
    "    labels_.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df for features and labels\n",
    "preprocessed_train_df = pd.DataFrame({'token_features': features, 'labels': labels_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(\n",
    "    preprocessed_train_df['token_features'], preprocessed_train_df['labels'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit CRF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "# Define the Conditional Random Fields model\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='l2sgd',\n",
    "    max_iterations=200,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    crf.fit(X_train, y_train)\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flat F1-score on evaluation data: 0.8122\n"
     ]
    }
   ],
   "source": [
    "from sklearn_crfsuite.metrics import flat_f1_score\n",
    "\n",
    "f1 = flat_f1_score(\n",
    "    y_eval, y_pred, average='weighted')\n",
    "print(f\"Flat F1-score on evaluation data: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the crf model on the whole dataset to get better results for the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='l2sgd',\n",
    "    max_iterations=200,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    crf.fit(preprocessed_train_df['token_features'],\n",
    "            preprocessed_train_df['labels'])\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Владелец «Бирмингема» получил шесть лет тюрьмы...</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Акция протеста на Майдане Независимости объявл...</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Фольксваген может перейти под контроль Порше \\...</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Москве покажут фильмы Чарли Чаплина с живой ...</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чулпан Хаматова сыграет главную роль в фильме ...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            senences   id\n",
       "0  Владелец «Бирмингема» получил шесть лет тюрьмы...  584\n",
       "1  Акция протеста на Майдане Независимости объявл...  585\n",
       "2  Фольксваген может перейти под контроль Порше \\...  586\n",
       "3  В Москве покажут фильмы Чарли Чаплина с живой ...  587\n",
       "4  Чулпан Хаматова сыграет главную роль в фильме ...  588"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json(path_or_buf='../data/test/test.jsonl', lines=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features for test data: 100%|██████████| 65/65 [00:06<00:00,  9.82it/s]\n"
     ]
    }
   ],
   "source": [
    "features_test = []\n",
    "# Iterating over each row in the test dataframe with a progress bar\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Extracting features for test data\"):\n",
    "    # Extracting the sentence from the row; note the typo in 'senences' which should be 'sentences'\n",
    "    sentence = row.senences\n",
    "    # Extracting features using a defined function and appending to the list\n",
    "    features_test.append(extract_features(sentence))\n",
    "\n",
    "# Converting the list of features into a pandas Series\n",
    "X_pred = pd.Series((feature for feature in features_test))\n",
    "# Making predictions using the CRF model\n",
    "y_pred = crf.predict(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Making Predictions for Test Data: 100%|██████████| 65/65 [00:06<00:00, 10.00it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for idx, (tokens, predicted_entities) in enumerate(tqdm(zip(test_df[\"senences\"], y_pred), total=len(y_pred), desc=\"Making Predictions for Test Data\")):\n",
    "    answer = []\n",
    "    current_start_index = None\n",
    "    current_entity = None\n",
    "\n",
    "    # Process each token and its corresponding predicted entity\n",
    "    for token, entity in zip(ru_nlp(tokens), predicted_entities):\n",
    "        if entity.startswith('B'):\n",
    "            # Beginning of a new entity\n",
    "            current_start_index = token.idx\n",
    "            current_end_index = token.idx + len(token.text) - 1\n",
    "            current_entity = entity[2:]  # Get the entity type\n",
    "            answer.append(\n",
    "                [current_start_index, current_end_index, current_entity])\n",
    "        elif entity.startswith('I') and current_start_index is not None:\n",
    "            # Inside an entity; update the end index of the last element\n",
    "            current_end_index = token.idx + len(token.text) - 1\n",
    "            # Update the last entity's end index\n",
    "            answer[-1][1] = current_end_index\n",
    "\n",
    "    predictions.append(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to test.jsonl file\n",
    "pd.DataFrame({'ners':  predictions, 'id': test_df[\"id\"]}).to_json('test.jsonl', orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: test.jsonl (deflated 73%)\n"
     ]
    }
   ],
   "source": [
    "!zip ./test.zip ./test.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
