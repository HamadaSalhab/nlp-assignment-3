{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we start by making a naive solution based on making a dictionary that stores the most frequent tags (ners) for each token (or multiple tokens if the entity consists of more than one word), and use it to predict the ners in the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ners</th>\n",
       "      <th>sentences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 5, CITY], [16, 23, PERSON], [34, 41, PERS...</td>\n",
       "      <td>Бостон взорвали Тамерлан и Джохар Царнаевы из ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[21, 28, PROFESSION], [53, 67, ORGANIZATION],...</td>\n",
       "      <td>Умер избитый до комы гитарист и сооснователь г...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 4, PERSON], [37, 42, COUNTRY], [47, 76, O...</td>\n",
       "      <td>Путин подписал распоряжение о выходе России из...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 11, PERSON], [36, 47, PROFESSION], [49, 6...</td>\n",
       "      <td>Бенедикт XVI носил кардиостимулятор\\nПапа Римс...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 4, PERSON], [17, 29, ORGANIZATION], [48, ...</td>\n",
       "      <td>Обама назначит в Верховный суд латиноамериканк...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ners  \\\n",
       "0  [[0, 5, CITY], [16, 23, PERSON], [34, 41, PERS...   \n",
       "1  [[21, 28, PROFESSION], [53, 67, ORGANIZATION],...   \n",
       "2  [[0, 4, PERSON], [37, 42, COUNTRY], [47, 76, O...   \n",
       "3  [[0, 11, PERSON], [36, 47, PROFESSION], [49, 6...   \n",
       "4  [[0, 4, PERSON], [17, 29, ORGANIZATION], [48, ...   \n",
       "\n",
       "                                           sentences  id  \n",
       "0  Бостон взорвали Тамерлан и Джохар Царнаевы из ...   0  \n",
       "1  Умер избитый до комы гитарист и сооснователь г...   1  \n",
       "2  Путин подписал распоряжение о выходе России из...   2  \n",
       "3  Бенедикт XVI носил кардиостимулятор\\nПапа Римс...   3  \n",
       "4  Обама назначит в Верховный суд латиноамериканк...   4  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = pd.read_json(\"../data/dev/train.jsonl\", lines=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the most frequent tags in the training dataset (sorted from most to least):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PERSON': 5119,\n",
       " 'PROFESSION': 5039,\n",
       " 'ORGANIZATION': 4088,\n",
       " 'EVENT': 3335,\n",
       " 'DATE': 2689,\n",
       " 'COUNTRY': 2510,\n",
       " 'CITY': 1261,\n",
       " 'NUMBER': 1107,\n",
       " 'AGE': 657,\n",
       " 'ORDINAL': 614,\n",
       " 'NATIONALITY': 437,\n",
       " 'FACILITY': 424,\n",
       " 'STATE_OR_PROVINCE': 412,\n",
       " 'LAW': 405,\n",
       " 'AWARD': 404,\n",
       " 'LOCATION': 314,\n",
       " 'IDEOLOGY': 273,\n",
       " 'WORK_OF_ART': 270,\n",
       " 'PRODUCT': 245,\n",
       " 'CRIME': 221,\n",
       " 'DISEASE': 220,\n",
       " 'TIME': 182,\n",
       " 'MONEY': 179,\n",
       " 'DISTRICT': 103,\n",
       " 'PENALTY': 92,\n",
       " 'RELIGION': 89,\n",
       " 'PERCENT': 68,\n",
       " 'LANGUAGE': 54,\n",
       " 'FAMILY': 24}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq = {}\n",
    "\n",
    "for ners in train_df[\"ners\"]:\n",
    "    for start, end, tag in ners:\n",
    "        if tag in tag_freq.keys():\n",
    "            tag_freq[tag] += 1\n",
    "        else:\n",
    "            tag_freq[tag] = 1\n",
    "\n",
    "sorted_tag_freq = dict(sorted(tag_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "print(\"These are the most frequent tags in the training dataset (sorted from most to least):\")\n",
    "sorted_tag_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary will have the words (tokens) as keys, and the their most frequent tag as values. We will get the most frequent tag for each word by simply counting how many times the word has appeared with the specific tag in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_most_frequent_tags(train_df):\n",
    "    most_freq_tag = {}\n",
    "\n",
    "    for sentence, ners in zip(train_df[\"sentences\"], train_df[\"ners\"]):\n",
    "        for start, end, tag in ners:\n",
    "\n",
    "            word = sentence[start:end+1]\n",
    "\n",
    "            if word in most_freq_tag:\n",
    "                most_freq_tag[word].append(tag)\n",
    "            else:\n",
    "                most_freq_tag[word] = [tag]\n",
    "\n",
    "    for key in most_freq_tag.keys():\n",
    "        tag, freq = Counter(most_freq_tag[key]).most_common()[0]\n",
    "\n",
    "        most_freq_tag[key] = tag\n",
    "    \n",
    "    return most_freq_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how the dictionary looks like (first 5 items):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Бостон': 'CITY',\n",
       " 'Тамерлан': 'PERSON',\n",
       " 'Царнаевы': 'PERSON',\n",
       " 'Северного Кавказа': 'LOCATION',\n",
       " 'спецоперация по поимке': 'EVENT'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_tag = get_most_frequent_tags(train_df)\n",
    "\n",
    "print(\"This is how the dictionary looks like (first 5 items):\")\n",
    "dict(list(most_freq_tag.items())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senences</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Владелец «Бирмингема» получил шесть лет тюрьмы...</td>\n",
       "      <td>584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Акция протеста на Майдане Независимости объявл...</td>\n",
       "      <td>585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Фольксваген может перейти под контроль Порше \\...</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В Москве покажут фильмы Чарли Чаплина с живой ...</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Чулпан Хаматова сыграет главную роль в фильме ...</td>\n",
       "      <td>588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            senences   id\n",
       "0  Владелец «Бирмингема» получил шесть лет тюрьмы...  584\n",
       "1  Акция протеста на Майдане Независимости объявл...  585\n",
       "2  Фольксваген может перейти под контроль Порше \\...  586\n",
       "3  В Москве покажут фильмы Чарли Чаплина с живой ...  587\n",
       "4  Чулпан Хаматова сыграет главную роль в фильме ...  588"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json(\"../data/test/test.jsonl\", lines=True)\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Prediction Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def tokenize_russian(text):\n",
    "    # Regular expression to match words and ignore punctuation\n",
    "    pattern = r'[^\\w\\s]|_'\n",
    "    # Replace punctuations with spaces before splitting\n",
    "    cleaned_text = re.sub(pattern, ' ', text)\n",
    "    # Split the text by whitespace and filter out any empty tokens\n",
    "    tokens = [token for token in cleaned_text.split() if token]\n",
    "    return tokens\n",
    "\n",
    "def generate_ngrams(tokens, n=4):\n",
    "    ngrams = []\n",
    "    for i in range(1, n+1):  # To generate n-grams for n=1 to 4\n",
    "        for j in range(len(tokens) - i + 1):\n",
    "            ngrams.append(' '.join(tokens[j:j+i]))\n",
    "    return ngrams\n",
    "\n",
    "\n",
    "def predict(df):\n",
    "    test_predictions = []\n",
    "\n",
    "    for id, sentence in zip(df[\"id\"], df[\"senences\"]):\n",
    "\n",
    "        # Tokenize the sentence\n",
    "        sentence_tokens = tokenize_russian(sentence)\n",
    "\n",
    "        index = 0\n",
    "\n",
    "        sentence_ners = []\n",
    "        for ngrams in generate_ngrams(sentence_tokens, n=29):\n",
    "            if ngrams in most_freq_tag:\n",
    "                # Start index of current ngram in the full sentence\n",
    "                start = index + sentence[index:].find(ngrams)\n",
    "                # End index of current ngram in the full sentence\n",
    "                end = start + len(ngrams) - 1\n",
    "                # Add prediction to current sentence's ners\n",
    "                sentence_ners.append((start, end, most_freq_tag[ngrams]))\n",
    "\n",
    "                # Increase the index by the predicted word(s) length\n",
    "                index += len(ngrams) + 1\n",
    "\n",
    "        test_predictions.append(sentence_ners)\n",
    "\n",
    "    return [{\"ners\": test_predictions[i], \"id\": test_df.iloc[i][\"id\"]} for i in range(len(test_df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ners': [(30, 34, 'NUMBER'),\n",
       "  (40, 45, 'PENALTY'),\n",
       "  (64, 69, 'PERSON'),\n",
       "  (128, 134, 'DATE'),\n",
       "  (137, 137, 'NUMBER'),\n",
       "  (145, 147, 'EVENT'),\n",
       "  (149, 156, 'STATE_OR_PROVINCE'),\n",
       "  (158, 167, 'EVENT'),\n",
       "  (298, 302, 'NUMBER'),\n",
       "  (320, 329, 'PENALTY'),\n",
       "  (350, 351, 'NUMBER'),\n",
       "  (382, 389, 'EVENT'),\n",
       "  (403, 404, 'NUMBER'),\n",
       "  (406, 414, 'NUMBER'),\n",
       "  (472, 475, 'DATE'),\n",
       "  (480, 483, 'DATE'),\n",
       "  (485, 488, 'DATE'),\n",
       "  (534, 537, 'DATE'),\n",
       "  (562, 563, 'NUMBER'),\n",
       "  (350, 350, 'NUMBER'),\n",
       "  (406, 414, 'NUMBER'),\n",
       "  (130, 138, 'DATE'),\n",
       "  (310, 329, 'PENALTY'),\n",
       "  (374, 389, 'EVENT'),\n",
       "  (532, 542, 'DATE')],\n",
       " 'id': 584}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions = predict(test_df)\n",
    "test_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Predictions to .jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Helper class to convert int64 integers to normal integers: see https://stackoverflow.com/questions/50916422/python-typeerror-object-of-type-int64-is-not-json-serializable\n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        return super(NpEncoder, self).default(obj)\n",
    "\n",
    "# Helper funcion to save the list of final predictions in a \".jsonl\" format\n",
    "def save_json_lines(data, path):\n",
    "    with open(path, 'w', encoding='utf-8') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file, ensure_ascii=False, cls=NpEncoder)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_lines(test_predictions, \"./test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: test.jsonl (deflated 78%)\n"
     ]
    }
   ],
   "source": [
    "!zip ./test.zip ./test.jsonl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".nlpenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
